# Job 4: Global closure - aggregate and send to Financial Lead via Outlook
resources:
  jobs:
    global_closure_send:
      name: "Getnet Closure - Global Closure and Send"
      description: "When all BUs valid, aggregate closure data, generate file, send via Outlook to Financial Lead."
      tasks:
        - task_key: "global_closure"
          timeout_seconds: 1800
          notebook_task:
            notebook_path: ../../src/notebooks/global_closure_send.py
            source: WORKSPACE
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              closure_period_type: ${var.closure_period_type}
          libraries:
            - pypi:
                package: msal
            - pypi:
                package: requests
          new_cluster:
            spark_version: "13.3.x-scala2.12"
            node_type_id: "Standard_DS3_v2"
            num_workers: 0
            data_security_mode: USER_ISOLATION
            spark_conf:
              spark.databricks.cluster.profile: singleNode
              spark.master: local[*]
            custom_tags:
              ResourceClass: SingleNode
          retry_on_timeout: true
          max_retries: 2
          min_retry_interval_millis: 60000
      schedule:
        quartz_cron_expression: "0 0 8 * * ?"  # daily 8 AM UTC
        timezone_id: "UTC"
